Env ID: [37]
================================================================================
Timestep: 0
State: tensor([4, 4, 0, 0, 0, 0])
Action: right
Reward: 0.9450391530990601
Distance: 6.590011119842529
Next state: tensor([5, 4, 0, 0, 0, 0])
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 0, 0, 0, 0])
Action: down
Reward: 2.4466540813446045
Distance: 5.544971942901611
Next state: tensor([5, 3, 0, 0, 0, 1])
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 3, 0, 0, 0, 1])
Action: down
Reward: 0.580720067024231
Distance: 2.9983179569244385
Next state: tensor([5, 2, 0, 0, 0, 0])
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 2, 0, 0, 0, 0])
Action: right
Reward: 2.2154579162597656
Distance: 2.3175978660583496
Next state: tensor([6, 2, 0, 0, 0, 0])
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 2, 0, 0, 0, 0])
Action: right
Reward: -0.09972353279590607
Distance: 0.0021399531979113817
Next state: tensor([7, 2, 0, 0, 0, 0])
================================================================================

================================================================================
Timestep: 5
State: tensor([7, 2, 0, 0, 0, 0])
Action: right
Reward: -0.10098503530025482
Distance: 0.0018634849693626165
Next state: tensor([8, 2, 0, 0, 0, 0])
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 2, 0, 0, 0, 0])
Action: end_episode
Reward: -0.09975625574588776
Distance: 0.0028485169168561697
Next state: tensor([8, 2, 0, 0, 0, 0])
================================================================================

