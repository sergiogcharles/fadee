Env ID: [9]
================================================================================
Timestep: 0
State: tensor([4, 4, 0, 0, 0])
Action: right
Reward: 0.11441125720739365
Distance: 6.890344142913818
Next state: tensor([5, 4, 1, 0, 0])
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1, 0, 0])
Action: right
Reward: 6.3795905113220215
Distance: 6.675932884216309
Next state: tensor([ 6,  4,  0, 10,  0])
================================================================================

================================================================================
Timestep: 2
State: tensor([ 6,  4,  0, 10,  0])
Action: left
Reward: 0.07578330487012863
Distance: 0.196342334151268
Next state: tensor([5, 4, 1, 0, 0])
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1, 0, 0])
Action: noop
Reward: -0.15552648901939392
Distance: 0.020559031516313553
Next state: tensor([5, 4, 1, 0, 0])
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1, 0, 0])
Action: right
Reward: -0.08063429594039917
Distance: 0.07608552277088165
Next state: tensor([ 6,  4,  0, 10,  0])
================================================================================

================================================================================
Timestep: 5
State: tensor([ 6,  4,  0, 10,  0])
Action: left
Reward: -0.12017741799354553
Distance: 0.0567198172211647
Next state: tensor([5, 4, 1, 0, 0])
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1, 0, 0])
Action: ride_bus
Reward: -0.3756430745124817
Distance: 0.07689723372459412
Next state: tensor([8, 1, 1, 0, 0])
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1, 0, 0])
Action: end_episode
Reward: -0.4191018044948578
Distance: 0.3525403141975403
Next state: tensor([8, 1, 1, 0, 0])
================================================================================

