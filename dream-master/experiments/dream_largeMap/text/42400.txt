Env ID: [296]
================================================================================
Timestep: 0
State: tensor([4, 4, 0, 0])
Action: pickup
Reward: -0.028118707239627838
Distance: 6.782334327697754
Next state: tensor([4, 4, 0, 0])
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0, 0])
Action: right
Reward: -0.10128412395715714
Distance: 6.710453033447266
Next state: tensor([5, 4, 1, 0])
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1, 0])
Action: right
Reward: 5.387829780578613
Distance: 6.711737155914307
Next state: tensor([  6,   4,   0, 297])
================================================================================

================================================================================
Timestep: 3
State: tensor([  6,   4,   0, 297])
Action: ride_bus
Reward: 0.439723938703537
Distance: 1.2239075899124146
Next state: tensor([  6,   4,   0, 297])
================================================================================

================================================================================
Timestep: 4
State: tensor([  6,   4,   0, 297])
Action: ride_bus
Reward: -0.08870402723550797
Distance: 0.684183657169342
Next state: tensor([  6,   4,   0, 297])
================================================================================

================================================================================
Timestep: 5
State: tensor([  6,   4,   0, 297])
Action: pickup
Reward: -0.119109608232975
Distance: 0.6728876829147339
Next state: tensor([  6,   4,   0, 297])
================================================================================

================================================================================
Timestep: 6
State: tensor([  6,   4,   0, 297])
Action: end_episode
Reward: -0.10878083854913712
Distance: 0.6919972896575928
Next state: tensor([  6,   4,   0, 297])
================================================================================

