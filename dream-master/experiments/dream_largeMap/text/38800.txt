Env ID: [527]
================================================================================
Timestep: 0
State: tensor([4, 4, 0, 0])
Action: right
Reward: -0.1176372542977333
Distance: 6.052743911743164
Next state: tensor([5, 4, 1, 0])
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1, 0])
Action: right
Reward: 3.917686939239502
Distance: 6.070381164550781
Next state: tensor([  6,   4,   0, 528])
================================================================================

================================================================================
Timestep: 2
State: tensor([  6,   4,   0, 528])
Action: ride_bus
Reward: 0.3514060080051422
Distance: 2.052694082260132
Next state: tensor([  6,   4,   0, 528])
================================================================================

================================================================================
Timestep: 3
State: tensor([  6,   4,   0, 528])
Action: noop
Reward: -0.12883707880973816
Distance: 1.601288080215454
Next state: tensor([  6,   4,   0, 528])
================================================================================

================================================================================
Timestep: 4
State: tensor([  6,   4,   0, 528])
Action: pickup
Reward: -0.15738555788993835
Distance: 1.6301251649856567
Next state: tensor([  6,   4,   0, 528])
================================================================================

================================================================================
Timestep: 5
State: tensor([  6,   4,   0, 528])
Action: pickup
Reward: -0.18212208151817322
Distance: 1.6875107288360596
Next state: tensor([  6,   4,   0, 528])
================================================================================

================================================================================
Timestep: 6
State: tensor([  6,   4,   0, 528])
Action: end_episode
Reward: -0.1732998788356781
Distance: 1.7696328163146973
Next state: tensor([  6,   4,   0, 528])
================================================================================

