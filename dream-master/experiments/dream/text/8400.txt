Env ID: [16]
================================================================================
Timestep: 0
State: tensor([4, 4, 0, 0])
Action: pickup
Reward: -0.2881241738796234
Distance: 7.7197418212890625
Next state: tensor([4, 4, 0, 0])
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0, 0])
Action: left
Reward: 0.179222971200943
Distance: 7.90786600112915
Next state: tensor([3, 4, 1, 0])
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1, 0])
Action: ride_bus
Reward: 1.0574458837509155
Distance: 7.628643035888672
Next state: tensor([0, 1, 1, 0])
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1, 0])
Action: noop
Reward: 1.1807669401168823
Distance: 6.471197128295898
Next state: tensor([0, 1, 1, 0])
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1, 0])
Action: noop
Reward: -0.12612542510032654
Distance: 5.190430164337158
Next state: tensor([0, 1, 1, 0])
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 1, 1, 0])
Action: right
Reward: -0.38999900221824646
Distance: 5.216555595397949
Next state: tensor([1, 1, 0, 0])
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 1, 0, 0])
Action: drop
Reward: 0.2640327513217926
Distance: 5.50655460357666
Next state: tensor([1, 1, 0, 0])
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 1, 0, 0])
Action: noop
Reward: -0.4279247224330902
Distance: 5.142521858215332
Next state: tensor([1, 1, 0, 0])
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 1, 0, 0])
Action: up
Reward: -0.8451076745986938
Distance: 5.470446586608887
Next state: tensor([1, 2, 0, 0])
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 2, 0, 0])
Action: drop
Reward: 0.09828033298254013
Distance: 6.215554237365723
Next state: tensor([1, 2, 0, 0])
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 2, 0, 0])
Action: right
Reward: -0.7062464952468872
Distance: 6.017273902893066
Next state: tensor([2, 2, 0, 0])
================================================================================

================================================================================
Timestep: 11
State: tensor([2, 2, 0, 0])
Action: up
Reward: -0.20461472868919373
Distance: 6.623520374298096
Next state: tensor([2, 3, 0, 0])
================================================================================

================================================================================
Timestep: 12
State: tensor([2, 3, 0, 0])
Action: down
Reward: -0.24986085295677185
Distance: 6.728135108947754
Next state: tensor([2, 2, 0, 0])
================================================================================

================================================================================
Timestep: 13
State: tensor([2, 2, 0, 0])
Action: noop
Reward: -0.3599935472011566
Distance: 6.87799596786499
Next state: tensor([2, 2, 0, 0])
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 2, 0, 0])
Action: ride_bus
Reward: -0.05860147625207901
Distance: 7.137989521026611
Next state: tensor([2, 2, 0, 0])
================================================================================

================================================================================
Timestep: 15
State: tensor([2, 2, 0, 0])
Action: pickup
Reward: -0.4462920129299164
Distance: 7.096590995788574
Next state: tensor([2, 2, 0, 0])
================================================================================

================================================================================
Timestep: 16
State: tensor([2, 2, 0, 0])
Action: end_episode
Reward: -0.3234778344631195
Distance: 7.442883014678955
Next state: tensor([2, 2, 0, 0])
================================================================================

